ssh://root@101.42.31.156:30003/root/anaconda3/envs/ztorch/bin/python -u /home/SQZ_project/DSTAGNN/train_DSTAGNN_my.py
Read configuration file: configurations/PEMS08_dstagnn.conf
CUDA: True cuda:0
folder_dir: dstagnn_h1d0w0_channel1_1.000000e-04
params_path: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04
load file: ./data/PEMS08/PEMS08_r1_d0_w0_dstagnn
train: torch.Size([10699, 170, 1, 12]) torch.Size([10699, 170, 12])
val: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12])
test: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12])
create params directory myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 4
nb_chev_filter	 32
nb_time_filter	 32
time_strides	 1
batch_size	 64
graph_signal_matrix_filename	 ./data/PEMS08/PEMS08.npz
start_epoch	 0
epochs	 100
DSTAGNN_submodule(
  (BlockList): ModuleList(
    (0): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 1), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 170)
        (norm): LayerNorm((170,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(170, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=170, out_features=96, bias=False)
        (W_K): Linear(in_features=170, out_features=96, bias=False)
        (W_V): Linear(in_features=170, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=170, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (1): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 170)
        (norm): LayerNorm((170,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(170, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=170, out_features=96, bias=False)
        (W_K): Linear(in_features=170, out_features=96, bias=False)
        (W_V): Linear(in_features=170, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=170, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (2): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 170)
        (norm): LayerNorm((170,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(170, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=170, out_features=96, bias=False)
        (W_K): Linear(in_features=170, out_features=96, bias=False)
        (W_V): Linear(in_features=170, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=170, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (3): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 170)
        (norm): LayerNorm((170,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(170, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=170, out_features=96, bias=False)
        (W_K): Linear(in_features=170, out_features=96, bias=False)
        (W_V): Linear(in_features=170, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=170, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 170x170 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(48, 128, kernel_size=(1, 32), stride=(1, 1))
  (final_fc): Linear(in_features=128, out_features=12, bias=True)
)
Net's state_dict:
BlockList.0.pre_conv.weight 	 torch.Size([512, 12, 1, 1])
BlockList.0.pre_conv.bias 	 torch.Size([512])
BlockList.0.EmbedT.pos_embed.weight 	 torch.Size([12, 170])
BlockList.0.EmbedT.norm.weight 	 torch.Size([170])
BlockList.0.EmbedT.norm.bias 	 torch.Size([170])
BlockList.0.EmbedS.pos_embed.weight 	 torch.Size([170, 512])
BlockList.0.EmbedS.norm.weight 	 torch.Size([512])
BlockList.0.EmbedS.norm.bias 	 torch.Size([512])
BlockList.0.TAt.W_Q.weight 	 torch.Size([96, 170])
BlockList.0.TAt.W_K.weight 	 torch.Size([96, 170])
BlockList.0.TAt.W_V.weight 	 torch.Size([96, 170])
BlockList.0.TAt.fc.weight 	 torch.Size([170, 96])
BlockList.0.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.0.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.mask.0 	 torch.Size([170, 170])
BlockList.0.cheb_conv_SAt.mask.1 	 torch.Size([170, 170])
BlockList.0.cheb_conv_SAt.mask.2 	 torch.Size([170, 170])
BlockList.0.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.0.gtu3.con2out.bias 	 torch.Size([64])
BlockList.0.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.0.gtu5.con2out.bias 	 torch.Size([64])
BlockList.0.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.0.gtu7.con2out.bias 	 torch.Size([64])
BlockList.0.residual_conv.weight 	 torch.Size([32, 1, 1, 1])
BlockList.0.residual_conv.bias 	 torch.Size([32])
BlockList.0.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.0.fcmy.0.bias 	 torch.Size([12])
BlockList.0.ln.weight 	 torch.Size([32])
BlockList.0.ln.bias 	 torch.Size([32])
BlockList.1.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.1.pre_conv.bias 	 torch.Size([512])
BlockList.1.EmbedT.pos_embed.weight 	 torch.Size([12, 170])
BlockList.1.EmbedT.norm.weight 	 torch.Size([170])
BlockList.1.EmbedT.norm.bias 	 torch.Size([170])
BlockList.1.EmbedS.pos_embed.weight 	 torch.Size([170, 512])
BlockList.1.EmbedS.norm.weight 	 torch.Size([512])
BlockList.1.EmbedS.norm.bias 	 torch.Size([512])
BlockList.1.TAt.W_Q.weight 	 torch.Size([96, 170])
BlockList.1.TAt.W_K.weight 	 torch.Size([96, 170])
BlockList.1.TAt.W_V.weight 	 torch.Size([96, 170])
BlockList.1.TAt.fc.weight 	 torch.Size([170, 96])
BlockList.1.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.1.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.mask.0 	 torch.Size([170, 170])
BlockList.1.cheb_conv_SAt.mask.1 	 torch.Size([170, 170])
BlockList.1.cheb_conv_SAt.mask.2 	 torch.Size([170, 170])
BlockList.1.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.1.gtu3.con2out.bias 	 torch.Size([64])
BlockList.1.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.1.gtu5.con2out.bias 	 torch.Size([64])
BlockList.1.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.1.gtu7.con2out.bias 	 torch.Size([64])
BlockList.1.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.1.residual_conv.bias 	 torch.Size([32])
BlockList.1.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.1.fcmy.0.bias 	 torch.Size([12])
BlockList.1.ln.weight 	 torch.Size([32])
BlockList.1.ln.bias 	 torch.Size([32])
BlockList.2.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.2.pre_conv.bias 	 torch.Size([512])
BlockList.2.EmbedT.pos_embed.weight 	 torch.Size([12, 170])
BlockList.2.EmbedT.norm.weight 	 torch.Size([170])
BlockList.2.EmbedT.norm.bias 	 torch.Size([170])
BlockList.2.EmbedS.pos_embed.weight 	 torch.Size([170, 512])
BlockList.2.EmbedS.norm.weight 	 torch.Size([512])
BlockList.2.EmbedS.norm.bias 	 torch.Size([512])
BlockList.2.TAt.W_Q.weight 	 torch.Size([96, 170])
BlockList.2.TAt.W_K.weight 	 torch.Size([96, 170])
BlockList.2.TAt.W_V.weight 	 torch.Size([96, 170])
BlockList.2.TAt.fc.weight 	 torch.Size([170, 96])
BlockList.2.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.2.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.2.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.mask.0 	 torch.Size([170, 170])
BlockList.2.cheb_conv_SAt.mask.1 	 torch.Size([170, 170])
BlockList.2.cheb_conv_SAt.mask.2 	 torch.Size([170, 170])
BlockList.2.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.2.gtu3.con2out.bias 	 torch.Size([64])
BlockList.2.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.2.gtu5.con2out.bias 	 torch.Size([64])
BlockList.2.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.2.gtu7.con2out.bias 	 torch.Size([64])
BlockList.2.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.2.residual_conv.bias 	 torch.Size([32])
BlockList.2.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.2.fcmy.0.bias 	 torch.Size([12])
BlockList.2.ln.weight 	 torch.Size([32])
BlockList.2.ln.bias 	 torch.Size([32])
BlockList.3.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.3.pre_conv.bias 	 torch.Size([512])
BlockList.3.EmbedT.pos_embed.weight 	 torch.Size([12, 170])
BlockList.3.EmbedT.norm.weight 	 torch.Size([170])
BlockList.3.EmbedT.norm.bias 	 torch.Size([170])
BlockList.3.EmbedS.pos_embed.weight 	 torch.Size([170, 512])
BlockList.3.EmbedS.norm.weight 	 torch.Size([512])
BlockList.3.EmbedS.norm.bias 	 torch.Size([512])
BlockList.3.TAt.W_Q.weight 	 torch.Size([96, 170])
BlockList.3.TAt.W_K.weight 	 torch.Size([96, 170])
BlockList.3.TAt.W_V.weight 	 torch.Size([96, 170])
BlockList.3.TAt.fc.weight 	 torch.Size([170, 96])
BlockList.3.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.3.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.3.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.mask.0 	 torch.Size([170, 170])
BlockList.3.cheb_conv_SAt.mask.1 	 torch.Size([170, 170])
BlockList.3.cheb_conv_SAt.mask.2 	 torch.Size([170, 170])
BlockList.3.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.3.gtu3.con2out.bias 	 torch.Size([64])
BlockList.3.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.3.gtu5.con2out.bias 	 torch.Size([64])
BlockList.3.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.3.gtu7.con2out.bias 	 torch.Size([64])
BlockList.3.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.3.residual_conv.bias 	 torch.Size([32])
BlockList.3.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.3.fcmy.0.bias 	 torch.Size([12])
BlockList.3.ln.weight 	 torch.Size([32])
BlockList.3.ln.bias 	 torch.Size([32])
final_conv.weight 	 torch.Size([128, 48, 1, 32])
final_conv.bias 	 torch.Size([128])
final_fc.weight 	 torch.Size([12, 128])
final_fc.bias 	 torch.Size([12])
Net's total params: 2296860
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]}]
current epoch:  0
validation batch 1 / 56, loss: 156.62
val loss 232.89378370557512
best epoch:  0
best val loss:  232.89378370557512
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_0.params
current epoch:  1
validation batch 1 / 56, loss: 59.84
val loss 121.84603561673846
best epoch:  1
best val loss:  121.84603561673846
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_1.params
current epoch:  2
validation batch 1 / 56, loss: 21.98
val loss 34.52746329988752
best epoch:  2
best val loss:  34.52746329988752
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_2.params
current epoch:  3
validation batch 1 / 56, loss: 17.82
val loss 22.39009972981044
best epoch:  3
best val loss:  22.39009972981044
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_3.params
current epoch:  4
validation batch 1 / 56, loss: 16.70
val loss 20.511103408677236
best epoch:  4
best val loss:  20.511103408677236
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_4.params
current epoch:  5
validation batch 1 / 56, loss: 16.00
val loss 19.822493842669896
best epoch:  5
best val loss:  19.822493842669896
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_5.params
global step: 1000, training loss: 20.24, time: 702.23s
current epoch:  6
validation batch 1 / 56, loss: 15.58
val loss 19.213464191981725
best epoch:  6
best val loss:  19.213464191981725
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_6.params
current epoch:  7
validation batch 1 / 56, loss: 14.88
val loss 18.783322027751378
best epoch:  7
best val loss:  18.783322027751378
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_7.params
current epoch:  8
validation batch 1 / 56, loss: 14.33
val loss 18.17354646750859
best epoch:  8
best val loss:  18.17354646750859
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_8.params
current epoch:  9
validation batch 1 / 56, loss: 13.84
val loss 17.943504895482743
best epoch:  9
best val loss:  17.943504895482743
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_9.params
current epoch:  10
validation batch 1 / 56, loss: 13.42
val loss 17.55785502706255
best epoch:  10
best val loss:  17.55785502706255
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_10.params
current epoch:  11
validation batch 1 / 56, loss: 13.24
val loss 17.516067590032304
best epoch:  11
best val loss:  17.516067590032304
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_11.params
global step: 2000, training loss: 17.44, time: 1432.19s
current epoch:  12
validation batch 1 / 56, loss: 13.07
val loss 17.271732432501658
best epoch:  12
best val loss:  17.271732432501658
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_12.params
current epoch:  13
validation batch 1 / 56, loss: 12.87
val loss 17.17453624520983
best epoch:  13
best val loss:  17.17453624520983
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_13.params
current epoch:  14
validation batch 1 / 56, loss: 12.99
val loss 17.00319904940469
best epoch:  14
best val loss:  17.00319904940469
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_14.params
current epoch:  15
validation batch 1 / 56, loss: 12.82
val loss 16.885732105800084
best epoch:  15
best val loss:  16.885732105800084
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_15.params
current epoch:  16
validation batch 1 / 56, loss: 12.69
val loss 16.84850079672677
best epoch:  16
best val loss:  16.84850079672677
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_16.params
current epoch:  17
validation batch 1 / 56, loss: 12.72
val loss 16.575425011771067
best epoch:  17
best val loss:  16.575425011771067
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_17.params
global step: 3000, training loss: 16.30, time: 2165.99s
current epoch:  18
validation batch 1 / 56, loss: 12.76
val loss 16.62196213858468
current epoch:  19
validation batch 1 / 56, loss: 12.56
val loss 16.565386652946472
best epoch:  19
best val loss:  16.565386652946472
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_19.params
current epoch:  20
validation batch 1 / 56, loss: 12.49
val loss 16.53355734688895
best epoch:  20
best val loss:  16.53355734688895
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_20.params
current epoch:  21
validation batch 1 / 56, loss: 12.54
val loss 16.49181537968772
best epoch:  21
best val loss:  16.49181537968772
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_21.params
current epoch:  22
validation batch 1 / 56, loss: 12.40
val loss 16.436502712113516
best epoch:  22
best val loss:  16.436502712113516
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_22.params
current epoch:  23
validation batch 1 / 56, loss: 12.40
val loss 16.44521221092769
global step: 4000, training loss: 15.39, time: 2900.85s
current epoch:  24
validation batch 1 / 56, loss: 12.34
val loss 16.313489896910532
best epoch:  24
best val loss:  16.313489896910532
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_24.params
current epoch:  25
validation batch 1 / 56, loss: 12.27
val loss 16.671666588102067
current epoch:  26
validation batch 1 / 56, loss: 12.20
val loss 16.365946326936996
current epoch:  27
validation batch 1 / 56, loss: 12.38
val loss 16.343577487128123
current epoch:  28
validation batch 1 / 56, loss: 12.07
val loss 16.526700445583888
current epoch:  29
validation batch 1 / 56, loss: 12.11
val loss 16.22597554751805
best epoch:  29
best val loss:  16.22597554751805
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_29.params
global step: 5000, training loss: 15.49, time: 3631.93s
current epoch:  30
validation batch 1 / 56, loss: 11.99
val loss 16.278944202831813
current epoch:  31
validation batch 1 / 56, loss: 12.29
val loss 16.349446518080576
current epoch:  32
validation batch 1 / 56, loss: 12.01
val loss 16.269729375839233
current epoch:  33
validation batch 1 / 56, loss: 11.98
val loss 16.228986416544235
current epoch:  34
validation batch 1 / 56, loss: 11.97
val loss 16.141734429768153
best epoch:  34
best val loss:  16.141734429768153
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_34.params
current epoch:  35
validation batch 1 / 56, loss: 11.93
val loss 16.26514322417123
global step: 6000, training loss: 15.20, time: 4360.10s
current epoch:  36
validation batch 1 / 56, loss: 11.89
val loss 16.148892555918014
current epoch:  37
validation batch 1 / 56, loss: 11.94
val loss 16.086509891918727
best epoch:  37
best val loss:  16.086509891918727
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_37.params
current epoch:  38
validation batch 1 / 56, loss: 11.92
val loss 16.070937701633998
best epoch:  38
best val loss:  16.070937701633998
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_38.params
current epoch:  39
validation batch 1 / 56, loss: 11.86
val loss 16.151236823626927
current epoch:  40
validation batch 1 / 56, loss: 11.96
val loss 16.121304869651794
current epoch:  41
validation batch 1 / 56, loss: 11.86
val loss 16.275886876242502
global step: 7000, training loss: 14.47, time: 5090.74s
current epoch:  42
validation batch 1 / 56, loss: 11.98
val loss 16.092389498438155
current epoch:  43
validation batch 1 / 56, loss: 11.81
val loss 16.237318788255966
current epoch:  44
validation batch 1 / 56, loss: 11.88
val loss 16.039361034120834
best epoch:  44
best val loss:  16.039361034120834
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_44.params
current epoch:  45
validation batch 1 / 56, loss: 11.80
val loss 16.027050971984863
best epoch:  45
best val loss:  16.027050971984863
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_45.params
current epoch:  46
validation batch 1 / 56, loss: 12.09
val loss 16.000622493880137
best epoch:  46
best val loss:  16.000622493880137
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_46.params
current epoch:  47
validation batch 1 / 56, loss: 11.94
val loss 16.02035289151328
global step: 8000, training loss: 14.12, time: 5819.57s
current epoch:  48
validation batch 1 / 56, loss: 11.82
val loss 15.940067682947431
best epoch:  48
best val loss:  15.940067682947431
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_48.params
current epoch:  49
validation batch 1 / 56, loss: 11.73
val loss 15.971184883798871
current epoch:  50
validation batch 1 / 56, loss: 12.01
val loss 15.908330576760429
best epoch:  50
best val loss:  15.908330576760429
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_50.params
current epoch:  51
validation batch 1 / 56, loss: 11.78
val loss 16.075953023774282
current epoch:  52
validation batch 1 / 56, loss: 11.87
val loss 15.923014112881251
current epoch:  53
validation batch 1 / 56, loss: 11.68
val loss 16.064428397587367
global step: 9000, training loss: 13.04, time: 6541.89s
current epoch:  54
validation batch 1 / 56, loss: 11.62
val loss 15.930687478610448
current epoch:  55
validation batch 1 / 56, loss: 11.71
val loss 15.915876865386963
current epoch:  56
validation batch 1 / 56, loss: 11.82
val loss 15.865048255239214
best epoch:  56
best val loss:  15.865048255239214
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_56.params
current epoch:  57
validation batch 1 / 56, loss: 11.71
val loss 15.994269575391497
current epoch:  58
validation batch 1 / 56, loss: 11.71
val loss 15.863831179482597
best epoch:  58
best val loss:  15.863831179482597
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_58.params
current epoch:  59
validation batch 1 / 56, loss: 11.69
val loss 15.854570065225873
best epoch:  59
best val loss:  15.854570065225873
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_59.params
global step: 10000, training loss: 14.13, time: 7138.02s
current epoch:  60
validation batch 1 / 56, loss: 11.62
val loss 15.887805308614459
current epoch:  61
validation batch 1 / 56, loss: 11.63
val loss 16.08086461680276
current epoch:  62
validation batch 1 / 56, loss: 11.60
val loss 15.842313936778478
best epoch:  62
best val loss:  15.842313936778478
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_62.params
current epoch:  63
validation batch 1 / 56, loss: 11.60
val loss 15.912588681493487
current epoch:  64
validation batch 1 / 56, loss: 11.59
val loss 15.885586704526629
current epoch:  65
validation batch 1 / 56, loss: 11.61
val loss 15.973916292190552
global step: 11000, training loss: 14.56, time: 7737.31s
current epoch:  66
validation batch 1 / 56, loss: 11.57
val loss 16.111957447869436
current epoch:  67
validation batch 1 / 56, loss: 11.84
val loss 15.839740599904742
best epoch:  67
best val loss:  15.839740599904742
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_67.params
current epoch:  68
validation batch 1 / 56, loss: 11.53
val loss 15.962870291301183
current epoch:  69
validation batch 1 / 56, loss: 11.69
val loss 15.99072974068778
current epoch:  70
validation batch 1 / 56, loss: 11.74
val loss 15.895116244043622
current epoch:  71
validation batch 1 / 56, loss: 11.51
val loss 15.983535851751055
global step: 12000, training loss: 13.48, time: 8335.24s
current epoch:  72
validation batch 1 / 56, loss: 11.63
val loss 15.896921583584376
current epoch:  73
validation batch 1 / 56, loss: 11.67
val loss 15.848653742245265
current epoch:  74
validation batch 1 / 56, loss: 11.57
val loss 15.875793082373482
current epoch:  75
validation batch 1 / 56, loss: 11.55
val loss 15.98614844254085
current epoch:  76
validation batch 1 / 56, loss: 11.56
val loss 15.937079123088292
current epoch:  77
validation batch 1 / 56, loss: 11.61
val loss 16.203293323516846
global step: 13000, training loss: 14.30, time: 8869.69s
current epoch:  78
validation batch 1 / 56, loss: 11.56
val loss 16.078021390097483
current epoch:  79
validation batch 1 / 56, loss: 11.81
val loss 15.831502641950335
best epoch:  79
best val loss:  15.831502641950335
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_79.params
current epoch:  80
validation batch 1 / 56, loss: 11.73
val loss 15.886005418641227
current epoch:  81
validation batch 1 / 56, loss: 11.60
val loss 15.986066664968218
current epoch:  82
validation batch 1 / 56, loss: 11.51
val loss 15.874752044677734
current epoch:  83
validation batch 1 / 56, loss: 11.72
val loss 15.969120229993548
global step: 14000, training loss: 14.53, time: 9371.16s
current epoch:  84
validation batch 1 / 56, loss: 11.63
val loss 16.00901574747903
current epoch:  85
validation batch 1 / 56, loss: 11.50
val loss 16.02681475026267
current epoch:  86
validation batch 1 / 56, loss: 11.85
val loss 15.860184107507978
current epoch:  87
validation batch 1 / 56, loss: 11.67
val loss 15.867568492889404
current epoch:  88
validation batch 1 / 56, loss: 11.59
val loss 15.892295513834272
current epoch:  89
validation batch 1 / 56, loss: 11.56
val loss 15.860285128865923
global step: 15000, training loss: 13.01, time: 9871.91s
current epoch:  90
validation batch 1 / 56, loss: 11.64
val loss 15.863696898732867
current epoch:  91
validation batch 1 / 56, loss: 11.69
val loss 15.837886009897504
current epoch:  92
validation batch 1 / 56, loss: 11.79
val loss 15.849744234766279
current epoch:  93
validation batch 1 / 56, loss: 11.69
val loss 15.809163349015373
best epoch:  93
best val loss:  15.809163349015373
save parameters to file: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_93.params
current epoch:  94
validation batch 1 / 56, loss: 11.64
val loss 15.853750603539604
current epoch:  95
validation batch 1 / 56, loss: 11.64
val loss 16.031386613845825
global step: 16000, training loss: 13.03, time: 10373.04s
current epoch:  96
validation batch 1 / 56, loss: 11.49
val loss 16.009987320218766
current epoch:  97
validation batch 1 / 56, loss: 11.48
val loss 15.989592756543841
current epoch:  98
validation batch 1 / 56, loss: 11.56
val loss 15.935810106141227
current epoch:  99
validation batch 1 / 56, loss: 11.55
val loss 15.872014130864825
best epoch: 93
load weight from: myexperiments/PEMS08/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_93.params
predicting data set batch 1 / 56
input: (3567, 170, 1, 12)
prediction: (3567, 170, 12)
data_target_tensor: (3567, 170, 12)
current epoch: 93, predict 1-th point
MAE: 13.85
RMSE: 21.45
MAPE: 8.83
current epoch: 93, predict 2-th point
MAE: 14.43
RMSE: 22.51
MAPE: 9.11
current epoch: 93, predict 3-th point
MAE: 14.86
RMSE: 23.29
MAPE: 9.36
current epoch: 93, predict 4-th point
MAE: 15.21
RMSE: 23.97
MAPE: 9.52
current epoch: 93, predict 5-th point
MAE: 15.60
RMSE: 24.66
MAPE: 9.78
current epoch: 93, predict 6-th point
MAE: 15.98
RMSE: 25.28
MAPE: 10.01
current epoch: 93, predict 7-th point
MAE: 16.29
RMSE: 25.77
MAPE: 10.23
current epoch: 93, predict 8-th point
MAE: 16.52
RMSE: 26.13
MAPE: 10.38
current epoch: 93, predict 9-th point
MAE: 16.68
RMSE: 26.41
MAPE: 10.52
current epoch: 93, predict 10-th point
MAE: 16.88
RMSE: 26.70
MAPE: 10.66
current epoch: 93, predict 11-th point
MAE: 17.28
RMSE: 27.26
MAPE: 10.89
current epoch: 93, predict 12-th point
MAE: 17.93
RMSE: 28.16
MAPE: 11.20
all MAE: 15.96
all RMSE: 25.21
all MAPE: 10.04
[13.847671, 21.453215330062925, 8.825433254241943, 14.428172, 22.507823346925537, 9.110662341117859, 14.86074, 23.29213383387525, 9.355823695659637, 15.213859, 23.96968580107682, 9.524073451757431, 15.60366, 24.66204433261895, 9.775415807962418, 15.9759865, 25.275702661242303, 10.014788061380386, 16.294786, 25.76732341658108, 10.229060798883438, 16.522022, 26.131931973207486, 10.377927869558334, 16.684301, 26.414047480400935, 10.524098575115204, 16.876219, 26.703667846661883, 10.663294047117233, 17.280294, 27.257343387694398, 10.893531143665314, 17.929232, 28.158477254791208, 11.202045530080795, 15.95974, 25.20661326022957, 10.0413478910923]

Process finished with exit code 0
