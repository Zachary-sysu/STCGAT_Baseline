ssh://root@101.42.31.156:30003/root/anaconda3/envs/ztorch/bin/python -u /home/SQZ_project/DSTAGNN/train_DSTAGNN_my.py
Read configuration file: configurations/PEMS04_dstagnn.conf
CUDA: True cuda:0
folder_dir: dstagnn_h1d0w0_channel1_1.000000e-04
params_path: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04
load file: ./data/PEMS04/PEMS04_r1_d0_w0_dstagnn
train: torch.Size([10181, 307, 1, 12]) torch.Size([10181, 307, 12])
val: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
test: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
delete the old one and create params directory myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 4
nb_chev_filter	 32
nb_time_filter	 32
time_strides	 1
batch_size	 32
graph_signal_matrix_filename	 ./data/PEMS04/PEMS04.npz
start_epoch	 0
epochs	 110
DSTAGNN_submodule(
  (BlockList): ModuleList(
    (0): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 1), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (1): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (2): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (3): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 307)
        (norm): LayerNorm((307,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(307, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=307, out_features=96, bias=False)
        (W_K): Linear(in_features=307, out_features=96, bias=False)
        (W_V): Linear(in_features=307, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=307, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 307x307 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(48, 128, kernel_size=(1, 32), stride=(1, 1))
  (final_fc): Linear(in_features=128, out_features=12, bias=True)
)
Net's state_dict:
BlockList.0.pre_conv.weight 	 torch.Size([512, 12, 1, 1])
BlockList.0.pre_conv.bias 	 torch.Size([512])
BlockList.0.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.0.EmbedT.norm.weight 	 torch.Size([307])
BlockList.0.EmbedT.norm.bias 	 torch.Size([307])
BlockList.0.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.0.EmbedS.norm.weight 	 torch.Size([512])
BlockList.0.EmbedS.norm.bias 	 torch.Size([512])
BlockList.0.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.0.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.0.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.0.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.0.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.0.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.0.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.0.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.0.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.0.gtu3.con2out.bias 	 torch.Size([64])
BlockList.0.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.0.gtu5.con2out.bias 	 torch.Size([64])
BlockList.0.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.0.gtu7.con2out.bias 	 torch.Size([64])
BlockList.0.residual_conv.weight 	 torch.Size([32, 1, 1, 1])
BlockList.0.residual_conv.bias 	 torch.Size([32])
BlockList.0.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.0.fcmy.0.bias 	 torch.Size([12])
BlockList.0.ln.weight 	 torch.Size([32])
BlockList.0.ln.bias 	 torch.Size([32])
BlockList.1.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.1.pre_conv.bias 	 torch.Size([512])
BlockList.1.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.1.EmbedT.norm.weight 	 torch.Size([307])
BlockList.1.EmbedT.norm.bias 	 torch.Size([307])
BlockList.1.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.1.EmbedS.norm.weight 	 torch.Size([512])
BlockList.1.EmbedS.norm.bias 	 torch.Size([512])
BlockList.1.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.1.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.1.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.1.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.1.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.1.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.1.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.1.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.1.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.1.gtu3.con2out.bias 	 torch.Size([64])
BlockList.1.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.1.gtu5.con2out.bias 	 torch.Size([64])
BlockList.1.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.1.gtu7.con2out.bias 	 torch.Size([64])
BlockList.1.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.1.residual_conv.bias 	 torch.Size([32])
BlockList.1.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.1.fcmy.0.bias 	 torch.Size([12])
BlockList.1.ln.weight 	 torch.Size([32])
BlockList.1.ln.bias 	 torch.Size([32])
BlockList.2.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.2.pre_conv.bias 	 torch.Size([512])
BlockList.2.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.2.EmbedT.norm.weight 	 torch.Size([307])
BlockList.2.EmbedT.norm.bias 	 torch.Size([307])
BlockList.2.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.2.EmbedS.norm.weight 	 torch.Size([512])
BlockList.2.EmbedS.norm.bias 	 torch.Size([512])
BlockList.2.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.2.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.2.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.2.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.2.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.2.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.2.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.2.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.2.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.2.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.2.gtu3.con2out.bias 	 torch.Size([64])
BlockList.2.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.2.gtu5.con2out.bias 	 torch.Size([64])
BlockList.2.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.2.gtu7.con2out.bias 	 torch.Size([64])
BlockList.2.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.2.residual_conv.bias 	 torch.Size([32])
BlockList.2.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.2.fcmy.0.bias 	 torch.Size([12])
BlockList.2.ln.weight 	 torch.Size([32])
BlockList.2.ln.bias 	 torch.Size([32])
BlockList.3.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.3.pre_conv.bias 	 torch.Size([512])
BlockList.3.EmbedT.pos_embed.weight 	 torch.Size([12, 307])
BlockList.3.EmbedT.norm.weight 	 torch.Size([307])
BlockList.3.EmbedT.norm.bias 	 torch.Size([307])
BlockList.3.EmbedS.pos_embed.weight 	 torch.Size([307, 512])
BlockList.3.EmbedS.norm.weight 	 torch.Size([512])
BlockList.3.EmbedS.norm.bias 	 torch.Size([512])
BlockList.3.TAt.W_Q.weight 	 torch.Size([96, 307])
BlockList.3.TAt.W_K.weight 	 torch.Size([96, 307])
BlockList.3.TAt.W_V.weight 	 torch.Size([96, 307])
BlockList.3.TAt.fc.weight 	 torch.Size([307, 96])
BlockList.3.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.3.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.3.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.mask.0 	 torch.Size([307, 307])
BlockList.3.cheb_conv_SAt.mask.1 	 torch.Size([307, 307])
BlockList.3.cheb_conv_SAt.mask.2 	 torch.Size([307, 307])
BlockList.3.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.3.gtu3.con2out.bias 	 torch.Size([64])
BlockList.3.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.3.gtu5.con2out.bias 	 torch.Size([64])
BlockList.3.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.3.gtu7.con2out.bias 	 torch.Size([64])
BlockList.3.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.3.residual_conv.bias 	 torch.Size([32])
BlockList.3.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.3.fcmy.0.bias 	 torch.Size([12])
BlockList.3.ln.weight 	 torch.Size([32])
BlockList.3.ln.bias 	 torch.Size([32])
final_conv.weight 	 torch.Size([128, 48, 1, 32])
final_conv.bias 	 torch.Size([128])
final_fc.weight 	 torch.Size([12, 128])
final_fc.bias 	 torch.Size([12])
Net's total params: 3579728
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]}]
current epoch:  0
validation batch 1 / 107, loss: 275.16
validation batch 101 / 107, loss: 331.34
val loss 217.58108520507812
best epoch:  0
best val loss:  217.58108520507812
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_0.params
current epoch:  1
validation batch 1 / 107, loss: 50.06
validation batch 101 / 107, loss: 76.00
val loss 47.42819316364894
best epoch:  1
best val loss:  47.42819316364894
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_1.params
current epoch:  2
validation batch 1 / 107, loss: 29.81
validation batch 101 / 107, loss: 35.04
val loss 26.344222737249925
best epoch:  2
best val loss:  26.344222737249925
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_2.params
current epoch:  3
validation batch 1 / 107, loss: 28.16
validation batch 101 / 107, loss: 31.33
val loss 24.283628811346038
best epoch:  3
best val loss:  24.283628811346038
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_3.params
global step: 1000, training loss: 26.19, time: 675.35s
current epoch:  4
validation batch 1 / 107, loss: 28.12
validation batch 101 / 107, loss: 30.26
val loss 23.348133719970132
best epoch:  4
best val loss:  23.348133719970132
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_4.params
current epoch:  5
validation batch 1 / 107, loss: 26.91
validation batch 101 / 107, loss: 29.57
val loss 21.935321237439307
best epoch:  5
best val loss:  21.935321237439307
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_5.params
current epoch:  6
validation batch 1 / 107, loss: 26.89
validation batch 101 / 107, loss: 28.94
val loss 21.618864344659254
best epoch:  6
best val loss:  21.618864344659254
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_6.params
global step: 2000, training loss: 19.22, time: 1341.38s
current epoch:  7
validation batch 1 / 107, loss: 26.29
validation batch 101 / 107, loss: 28.40
val loss 21.3361391976615
best epoch:  7
best val loss:  21.3361391976615
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_7.params
current epoch:  8
validation batch 1 / 107, loss: 25.58
validation batch 101 / 107, loss: 28.16
val loss 20.55084816763334
best epoch:  8
best val loss:  20.55084816763334
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_8.params
current epoch:  9
validation batch 1 / 107, loss: 25.45
validation batch 101 / 107, loss: 27.91
val loss 20.462231199318005
best epoch:  9
best val loss:  20.462231199318005
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_9.params
global step: 3000, training loss: 20.21, time: 2014.72s
current epoch:  10
validation batch 1 / 107, loss: 25.08
validation batch 101 / 107, loss: 27.73
val loss 20.159472318453208
best epoch:  10
best val loss:  20.159472318453208
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_10.params
current epoch:  11
validation batch 1 / 107, loss: 25.10
validation batch 101 / 107, loss: 27.66
val loss 19.931628837763707
best epoch:  11
best val loss:  19.931628837763707
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_11.params
current epoch:  12
validation batch 1 / 107, loss: 24.85
validation batch 101 / 107, loss: 27.41
val loss 19.949061224393756
global step: 4000, training loss: 19.23, time: 2685.98s
current epoch:  13
validation batch 1 / 107, loss: 24.80
validation batch 101 / 107, loss: 27.41
val loss 19.95581611740255
current epoch:  14
validation batch 1 / 107, loss: 24.79
validation batch 101 / 107, loss: 27.49
val loss 19.614351985610533
best epoch:  14
best val loss:  19.614351985610533
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_14.params
current epoch:  15
validation batch 1 / 107, loss: 24.60
validation batch 101 / 107, loss: 27.25
val loss 19.536218139612785
best epoch:  15
best val loss:  19.536218139612785
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_15.params
global step: 5000, training loss: 19.67, time: 3313.29s
current epoch:  16
validation batch 1 / 107, loss: 25.07
validation batch 101 / 107, loss: 27.23
val loss 19.809368703966943
current epoch:  17
validation batch 1 / 107, loss: 24.73
validation batch 101 / 107, loss: 27.17
val loss 19.663280081526143
current epoch:  18
validation batch 1 / 107, loss: 24.36
validation batch 101 / 107, loss: 27.02
val loss 19.342867049101358
best epoch:  18
best val loss:  19.342867049101358
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_18.params
global step: 6000, training loss: 19.75, time: 3890.09s
current epoch:  19
validation batch 1 / 107, loss: 24.23
validation batch 101 / 107, loss: 26.97
val loss 19.290379011742424
best epoch:  19
best val loss:  19.290379011742424
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_19.params
current epoch:  20
validation batch 1 / 107, loss: 24.49
validation batch 101 / 107, loss: 26.96
val loss 19.471032913600173
current epoch:  21
validation batch 1 / 107, loss: 24.44
validation batch 101 / 107, loss: 27.04
val loss 19.182698909367357
best epoch:  21
best val loss:  19.182698909367357
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_21.params
global step: 7000, training loss: 16.10, time: 4537.07s
current epoch:  22
validation batch 1 / 107, loss: 24.34
validation batch 101 / 107, loss: 26.96
val loss 19.33070421218872
current epoch:  23
validation batch 1 / 107, loss: 24.15
validation batch 101 / 107, loss: 26.95
val loss 19.105176194805964
best epoch:  23
best val loss:  19.105176194805964
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_23.params
current epoch:  24
validation batch 1 / 107, loss: 24.29
validation batch 101 / 107, loss: 26.84
val loss 19.175622432031364
current epoch:  25
validation batch 1 / 107, loss: 24.16
validation batch 101 / 107, loss: 27.08
val loss 19.060283019163897
best epoch:  25
best val loss:  19.060283019163897
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_25.params
global step: 8000, training loss: 17.18, time: 5222.40s
current epoch:  26
validation batch 1 / 107, loss: 24.23
validation batch 101 / 107, loss: 26.84
val loss 19.091453423009856
current epoch:  27
validation batch 1 / 107, loss: 24.15
validation batch 101 / 107, loss: 26.89
val loss 18.869968877774532
best epoch:  27
best val loss:  18.869968877774532
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_27.params
current epoch:  28
validation batch 1 / 107, loss: 24.04
validation batch 101 / 107, loss: 26.74
val loss 18.956867815178132
global step: 9000, training loss: 17.96, time: 5894.69s
current epoch:  29
validation batch 1 / 107, loss: 24.10
validation batch 101 / 107, loss: 27.01
val loss 18.96541852148894
current epoch:  30
validation batch 1 / 107, loss: 24.13
validation batch 101 / 107, loss: 27.04
val loss 19.048427862541697
current epoch:  31
validation batch 1 / 107, loss: 24.20
validation batch 101 / 107, loss: 27.02
val loss 19.17851480590963
global step: 10000, training loss: 18.50, time: 6684.70s
current epoch:  32
validation batch 1 / 107, loss: 23.92
validation batch 101 / 107, loss: 26.88
val loss 18.835724429549458
best epoch:  32
best val loss:  18.835724429549458
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_32.params
current epoch:  33
validation batch 1 / 107, loss: 23.95
validation batch 101 / 107, loss: 26.97
val loss 18.93646224636898
current epoch:  34
validation batch 1 / 107, loss: 24.03
validation batch 101 / 107, loss: 27.06
val loss 18.93979274446719
global step: 11000, training loss: 18.89, time: 7481.24s
current epoch:  35
validation batch 1 / 107, loss: 24.02
validation batch 101 / 107, loss: 27.32
val loss 19.015414808398095
current epoch:  36
validation batch 1 / 107, loss: 23.93
validation batch 101 / 107, loss: 27.14
val loss 18.70171118228235
best epoch:  36
best val loss:  18.70171118228235
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_36.params
current epoch:  37
validation batch 1 / 107, loss: 23.86
validation batch 101 / 107, loss: 26.97
val loss 18.758829923433677
global step: 12000, training loss: 15.93, time: 8250.87s
current epoch:  38
validation batch 1 / 107, loss: 23.97
validation batch 101 / 107, loss: 27.07
val loss 18.724191968686114
current epoch:  39
validation batch 1 / 107, loss: 23.79
validation batch 101 / 107, loss: 27.25
val loss 18.835121132503048
current epoch:  40
validation batch 1 / 107, loss: 23.83
validation batch 101 / 107, loss: 27.05
val loss 18.77653101894343
global step: 13000, training loss: 14.54, time: 9018.94s
current epoch:  41
validation batch 1 / 107, loss: 23.79
validation batch 101 / 107, loss: 27.31
val loss 18.705517265284172
current epoch:  42
validation batch 1 / 107, loss: 23.84
validation batch 101 / 107, loss: 27.25
val loss 18.714920021663204
current epoch:  43
validation batch 1 / 107, loss: 23.96
validation batch 101 / 107, loss: 27.46
val loss 18.75414044834743
global step: 14000, training loss: 18.35, time: 9788.47s
current epoch:  44
validation batch 1 / 107, loss: 23.72
validation batch 101 / 107, loss: 27.23
val loss 18.643368079283526
best epoch:  44
best val loss:  18.643368079283526
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_44.params
current epoch:  45
validation batch 1 / 107, loss: 23.83
validation batch 101 / 107, loss: 27.38
val loss 18.72869385514304
current epoch:  46
validation batch 1 / 107, loss: 23.84
validation batch 101 / 107, loss: 27.40
val loss 18.784825538920465
current epoch:  47
validation batch 1 / 107, loss: 23.77
validation batch 101 / 107, loss: 27.44
val loss 18.57373597243122
best epoch:  47
best val loss:  18.57373597243122
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_47.params
global step: 15000, training loss: 17.75, time: 10508.19s
current epoch:  48
validation batch 1 / 107, loss: 23.70
validation batch 101 / 107, loss: 27.35
val loss 18.693551379943564
current epoch:  49
validation batch 1 / 107, loss: 23.81
validation batch 101 / 107, loss: 27.37
val loss 18.728440547657904
current epoch:  50
validation batch 1 / 107, loss: 23.77
validation batch 101 / 107, loss: 27.34
val loss 18.65528030484636
global step: 16000, training loss: 16.49, time: 11247.48s
current epoch:  51
validation batch 1 / 107, loss: 23.76
validation batch 101 / 107, loss: 27.44
val loss 18.676963743762435
current epoch:  52
validation batch 1 / 107, loss: 23.96
validation batch 101 / 107, loss: 27.75
val loss 19.0173813605977
current epoch:  53
validation batch 1 / 107, loss: 23.71
validation batch 101 / 107, loss: 27.54
val loss 18.625802031187252
global step: 17000, training loss: 17.50, time: 12017.45s
current epoch:  54
validation batch 1 / 107, loss: 23.71
validation batch 101 / 107, loss: 27.53
val loss 18.734042265704858
current epoch:  55
validation batch 1 / 107, loss: 23.66
validation batch 101 / 107, loss: 27.49
val loss 18.571115631923497
best epoch:  55
best val loss:  18.571115631923497
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_55.params
current epoch:  56
validation batch 1 / 107, loss: 23.70
validation batch 101 / 107, loss: 27.38
val loss 18.6473854546235
global step: 18000, training loss: 16.04, time: 12711.64s
current epoch:  57
validation batch 1 / 107, loss: 23.67
validation batch 101 / 107, loss: 27.64
val loss 18.592695258488163
current epoch:  58
validation batch 1 / 107, loss: 23.61
validation batch 101 / 107, loss: 27.50
val loss 18.63425577466733
current epoch:  59
validation batch 1 / 107, loss: 23.81
validation batch 101 / 107, loss: 27.68
val loss 18.810647986759648
global step: 19000, training loss: 17.59, time: 13375.25s
current epoch:  60
validation batch 1 / 107, loss: 23.58
validation batch 101 / 107, loss: 27.63
val loss 18.728389472604913
current epoch:  61
validation batch 1 / 107, loss: 23.52
validation batch 101 / 107, loss: 27.55
val loss 18.59591481395971
current epoch:  62
validation batch 1 / 107, loss: 23.59
validation batch 101 / 107, loss: 27.88
val loss 18.57828015924614
global step: 20000, training loss: 14.30, time: 14037.90s
current epoch:  63
validation batch 1 / 107, loss: 23.67
validation batch 101 / 107, loss: 27.69
val loss 18.57572879078232
current epoch:  64
validation batch 1 / 107, loss: 23.71
validation batch 101 / 107, loss: 27.68
val loss 18.639649097050462
current epoch:  65
validation batch 1 / 107, loss: 23.66
validation batch 101 / 107, loss: 27.51
val loss 18.61150233990678
global step: 21000, training loss: 16.32, time: 14696.61s
current epoch:  66
validation batch 1 / 107, loss: 23.70
validation batch 101 / 107, loss: 27.81
val loss 18.602013316109915
current epoch:  67
validation batch 1 / 107, loss: 23.62
validation batch 101 / 107, loss: 27.67
val loss 18.55144863930818
best epoch:  67
best val loss:  18.55144863930818
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_67.params
current epoch:  68
validation batch 1 / 107, loss: 23.63
validation batch 101 / 107, loss: 27.98
val loss 18.564756455822526
global step: 22000, training loss: 16.53, time: 15359.98s
current epoch:  69
validation batch 1 / 107, loss: 23.61
validation batch 101 / 107, loss: 27.73
val loss 18.564683201157045
current epoch:  70
validation batch 1 / 107, loss: 23.48
validation batch 101 / 107, loss: 27.81
val loss 18.58039204428129
current epoch:  71
validation batch 1 / 107, loss: 23.91
validation batch 101 / 107, loss: 27.89
val loss 18.572022282074546
current epoch:  72
validation batch 1 / 107, loss: 23.71
validation batch 101 / 107, loss: 27.66
val loss 18.55572894800489
global step: 23000, training loss: 16.50, time: 16025.83s
current epoch:  73
validation batch 1 / 107, loss: 23.64
validation batch 101 / 107, loss: 27.86
val loss 18.61191823995002
current epoch:  74
validation batch 1 / 107, loss: 23.67
validation batch 101 / 107, loss: 27.86
val loss 18.602244577675222
current epoch:  75
validation batch 1 / 107, loss: 23.79
validation batch 101 / 107, loss: 27.93
val loss 18.605860714600464
global step: 24000, training loss: 16.81, time: 16602.48s
current epoch:  76
validation batch 1 / 107, loss: 23.69
validation batch 101 / 107, loss: 28.01
val loss 18.64541642465324
current epoch:  77
validation batch 1 / 107, loss: 23.69
validation batch 101 / 107, loss: 27.85
val loss 18.77993740990897
current epoch:  78
validation batch 1 / 107, loss: 23.86
validation batch 101 / 107, loss: 28.07
val loss 18.73601189729209
global step: 25000, training loss: 17.01, time: 17179.27s
current epoch:  79
validation batch 1 / 107, loss: 23.66
validation batch 101 / 107, loss: 27.83
val loss 18.619579751914905
current epoch:  80
validation batch 1 / 107, loss: 23.74
validation batch 101 / 107, loss: 27.90
val loss 18.539475111203775
best epoch:  80
best val loss:  18.539475111203775
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_80.params
current epoch:  81
validation batch 1 / 107, loss: 23.60
validation batch 101 / 107, loss: 27.93
val loss 18.537387290847636
best epoch:  81
best val loss:  18.537387290847636
save parameters to file: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_81.params
global step: 26000, training loss: 15.76, time: 17756.75s
current epoch:  82
validation batch 1 / 107, loss: 23.60
validation batch 101 / 107, loss: 27.94
val loss 18.59721300980755
current epoch:  83
validation batch 1 / 107, loss: 23.59
validation batch 101 / 107, loss: 27.96
val loss 18.571320765486387
current epoch:  84
validation batch 1 / 107, loss: 23.66
validation batch 101 / 107, loss: 27.91
val loss 18.546099818755533
global step: 27000, training loss: 15.43, time: 18333.79s
current epoch:  85
validation batch 1 / 107, loss: 23.73
validation batch 101 / 107, loss: 28.03
val loss 18.61654100685476
current epoch:  86
validation batch 1 / 107, loss: 23.65
validation batch 101 / 107, loss: 27.97
val loss 18.57794812015284
current epoch:  87
validation batch 1 / 107, loss: 23.63
validation batch 101 / 107, loss: 28.18
val loss 18.76977285937728
global step: 28000, training loss: 17.98, time: 18910.63s
current epoch:  88
validation batch 1 / 107, loss: 23.73
validation batch 101 / 107, loss: 28.14
val loss 18.654606453726224
current epoch:  89
validation batch 1 / 107, loss: 23.61
validation batch 101 / 107, loss: 28.22
val loss 18.643545315644452
current epoch:  90
validation batch 1 / 107, loss: 23.58
validation batch 101 / 107, loss: 28.11
val loss 18.65703187479037
global step: 29000, training loss: 15.24, time: 19487.27s
current epoch:  91
validation batch 1 / 107, loss: 23.62
validation batch 101 / 107, loss: 28.32
val loss 18.64212693009421
current epoch:  92
validation batch 1 / 107, loss: 23.58
validation batch 101 / 107, loss: 28.10
val loss 18.594083884052026
current epoch:  93
validation batch 1 / 107, loss: 23.69
validation batch 101 / 107, loss: 28.67
val loss 18.716744159983698
current epoch:  94
validation batch 1 / 107, loss: 23.47
validation batch 101 / 107, loss: 28.19
val loss 18.555944126343057
global step: 30000, training loss: 16.35, time: 20075.34s
current epoch:  95
validation batch 1 / 107, loss: 23.42
validation batch 101 / 107, loss: 28.05
val loss 18.68934104152929
current epoch:  96
validation batch 1 / 107, loss: 23.69
validation batch 101 / 107, loss: 28.18
val loss 18.650166667510415
current epoch:  97
validation batch 1 / 107, loss: 23.58
validation batch 101 / 107, loss: 28.53
val loss 18.64433525210229
global step: 31000, training loss: 14.78, time: 20652.25s
current epoch:  98
validation batch 1 / 107, loss: 23.60
validation batch 101 / 107, loss: 28.35
val loss 18.6386853824152
current epoch:  99
validation batch 1 / 107, loss: 23.63
validation batch 101 / 107, loss: 28.04
val loss 18.592226095288712
current epoch:  100
validation batch 1 / 107, loss: 23.58
validation batch 101 / 107, loss: 28.45
val loss 18.749608980161007
global step: 32000, training loss: 17.01, time: 21229.27s
current epoch:  101
validation batch 1 / 107, loss: 23.51
validation batch 101 / 107, loss: 28.19
val loss 18.62366087637215
current epoch:  102
validation batch 1 / 107, loss: 23.60
validation batch 101 / 107, loss: 28.30
val loss 18.630595430035456
current epoch:  103
validation batch 1 / 107, loss: 23.56
validation batch 101 / 107, loss: 28.39
val loss 18.675111244772083
global step: 33000, training loss: 16.26, time: 21805.68s
current epoch:  104
validation batch 1 / 107, loss: 23.58
validation batch 101 / 107, loss: 28.35
val loss 18.614739333357765
current epoch:  105
validation batch 1 / 107, loss: 23.56
validation batch 101 / 107, loss: 28.38
val loss 18.640530296575243
current epoch:  106
validation batch 1 / 107, loss: 23.56
validation batch 101 / 107, loss: 28.54
val loss 18.683482905414618
global step: 34000, training loss: 16.19, time: 22382.42s
current epoch:  107
validation batch 1 / 107, loss: 23.63
validation batch 101 / 107, loss: 28.30
val loss 18.637402761762388
current epoch:  108
validation batch 1 / 107, loss: 23.64
validation batch 101 / 107, loss: 28.41
val loss 18.636556331242357
current epoch:  109
validation batch 1 / 107, loss: 23.54
validation batch 101 / 107, loss: 28.31
val loss 18.669287623646103
global step: 35000, training loss: 16.25, time: 22959.08s
best epoch: 81
load weight from: myexperiments/PEMS04/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_81.params
predicting data set batch 1 / 107
predicting data set batch 101 / 107
input: (3394, 307, 1, 12)
prediction: (3394, 307, 12)
data_target_tensor: (3394, 307, 12)
current epoch: 81, predict 1-th point
MAE: 17.36
RMSE: 27.97
MAPE: 11.65
current epoch: 81, predict 2-th point
MAE: 17.97
RMSE: 29.08
MAPE: 11.99
current epoch: 81, predict 3-th point
MAE: 18.41
RMSE: 29.92
MAPE: 12.21
current epoch: 81, predict 4-th point
MAE: 18.76
RMSE: 30.60
MAPE: 12.33
current epoch: 81, predict 5-th point
MAE: 19.09
RMSE: 31.22
MAPE: 12.51
current epoch: 81, predict 6-th point
MAE: 19.42
RMSE: 31.81
MAPE: 12.72
current epoch: 81, predict 7-th point
MAE: 19.76
RMSE: 32.35
MAPE: 12.91
current epoch: 81, predict 8-th point
MAE: 20.00
RMSE: 32.75
MAPE: 13.07
current epoch: 81, predict 9-th point
MAE: 20.17
RMSE: 33.08
MAPE: 13.20
current epoch: 81, predict 10-th point
MAE: 20.34
RMSE: 33.34
MAPE: 13.32
current epoch: 81, predict 11-th point
MAE: 20.61
RMSE: 33.74
MAPE: 13.53
current epoch: 81, predict 12-th point
MAE: 21.13
RMSE: 34.42
MAPE: 13.89
all MAE: 19.42
all RMSE: 31.75
all MAPE: 12.78
[17.36214, 27.973405469735336, 11.652892082929611, 17.966358, 29.081226115869732, 11.985210329294205, 18.405579, 29.917439088616756, 12.211103737354279, 18.756325, 30.60112370441086, 12.332071363925934, 19.087807, 31.220037393688067, 12.511363625526428, 19.415226, 31.809547152373334, 12.72236555814743, 19.75879, 32.3519954767573, 12.907548248767853, 19.995127, 32.75483213695824, 13.065320253372192, 20.172668, 33.07522793761216, 13.195082545280457, 20.33754, 33.341123153908036, 13.319255411624908, 20.614637, 33.742888428989744, 13.533647358417511, 21.126373, 34.41830014346474, 13.892652094364166, 19.41656, 31.747161498055124, 12.777398526668549]

Process finished with exit code 0
