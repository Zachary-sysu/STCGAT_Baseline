ssh://root@101.42.31.156:30003/root/anaconda3/envs/ztorch/bin/python -u /home/SQZ_project/DSTAGNN/train_DSTAGNN_my.py
Read configuration file: configurations/PEMS03_dstagnn.conf
CUDA: True cuda:0
folder_dir: dstagnn_h1d0w0_channel1_1.000000e-04
params_path: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04
load file: ./data/PEMS03/PEMS03_r1_d0_w0_dstagnn
train: torch.Size([15711, 358, 1, 12]) torch.Size([15711, 358, 12])
val: torch.Size([5237, 358, 1, 12]) torch.Size([5237, 358, 12])
test: torch.Size([5237, 358, 1, 12]) torch.Size([5237, 358, 12])
delete the old one and create params directory myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 4
nb_chev_filter	 32
nb_time_filter	 32
time_strides	 1
batch_size	 32
graph_signal_matrix_filename	 ./data/PEMS03/PEMS03.npz
start_epoch	 0
epochs	 90
DSTAGNN_submodule(
  (BlockList): ModuleList(
    (0): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 1), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 358)
        (norm): LayerNorm((358,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(358, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=358, out_features=96, bias=False)
        (W_K): Linear(in_features=358, out_features=96, bias=False)
        (W_V): Linear(in_features=358, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=358, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (1): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 358)
        (norm): LayerNorm((358,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(358, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=358, out_features=96, bias=False)
        (W_K): Linear(in_features=358, out_features=96, bias=False)
        (W_V): Linear(in_features=358, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=358, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (2): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 358)
        (norm): LayerNorm((358,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(358, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=358, out_features=96, bias=False)
        (W_K): Linear(in_features=358, out_features=96, bias=False)
        (W_V): Linear(in_features=358, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=358, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (3): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 358)
        (norm): LayerNorm((358,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(358, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=358, out_features=96, bias=False)
        (W_K): Linear(in_features=358, out_features=96, bias=False)
        (W_V): Linear(in_features=358, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=358, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(48, 128, kernel_size=(1, 32), stride=(1, 1))
  (final_fc): Linear(in_features=128, out_features=12, bias=True)
)
Net's state_dict:
BlockList.0.pre_conv.weight 	 torch.Size([512, 12, 1, 1])
BlockList.0.pre_conv.bias 	 torch.Size([512])
BlockList.0.EmbedT.pos_embed.weight 	 torch.Size([12, 358])
BlockList.0.EmbedT.norm.weight 	 torch.Size([358])
BlockList.0.EmbedT.norm.bias 	 torch.Size([358])
BlockList.0.EmbedS.pos_embed.weight 	 torch.Size([358, 512])
BlockList.0.EmbedS.norm.weight 	 torch.Size([512])
BlockList.0.EmbedS.norm.bias 	 torch.Size([512])
BlockList.0.TAt.W_Q.weight 	 torch.Size([96, 358])
BlockList.0.TAt.W_K.weight 	 torch.Size([96, 358])
BlockList.0.TAt.W_V.weight 	 torch.Size([96, 358])
BlockList.0.TAt.fc.weight 	 torch.Size([358, 96])
BlockList.0.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.0.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.mask.0 	 torch.Size([358, 358])
BlockList.0.cheb_conv_SAt.mask.1 	 torch.Size([358, 358])
BlockList.0.cheb_conv_SAt.mask.2 	 torch.Size([358, 358])
BlockList.0.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.0.gtu3.con2out.bias 	 torch.Size([64])
BlockList.0.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.0.gtu5.con2out.bias 	 torch.Size([64])
BlockList.0.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.0.gtu7.con2out.bias 	 torch.Size([64])
BlockList.0.residual_conv.weight 	 torch.Size([32, 1, 1, 1])
BlockList.0.residual_conv.bias 	 torch.Size([32])
BlockList.0.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.0.fcmy.0.bias 	 torch.Size([12])
BlockList.0.ln.weight 	 torch.Size([32])
BlockList.0.ln.bias 	 torch.Size([32])
BlockList.1.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.1.pre_conv.bias 	 torch.Size([512])
BlockList.1.EmbedT.pos_embed.weight 	 torch.Size([12, 358])
BlockList.1.EmbedT.norm.weight 	 torch.Size([358])
BlockList.1.EmbedT.norm.bias 	 torch.Size([358])
BlockList.1.EmbedS.pos_embed.weight 	 torch.Size([358, 512])
BlockList.1.EmbedS.norm.weight 	 torch.Size([512])
BlockList.1.EmbedS.norm.bias 	 torch.Size([512])
BlockList.1.TAt.W_Q.weight 	 torch.Size([96, 358])
BlockList.1.TAt.W_K.weight 	 torch.Size([96, 358])
BlockList.1.TAt.W_V.weight 	 torch.Size([96, 358])
BlockList.1.TAt.fc.weight 	 torch.Size([358, 96])
BlockList.1.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.1.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.mask.0 	 torch.Size([358, 358])
BlockList.1.cheb_conv_SAt.mask.1 	 torch.Size([358, 358])
BlockList.1.cheb_conv_SAt.mask.2 	 torch.Size([358, 358])
BlockList.1.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.1.gtu3.con2out.bias 	 torch.Size([64])
BlockList.1.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.1.gtu5.con2out.bias 	 torch.Size([64])
BlockList.1.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.1.gtu7.con2out.bias 	 torch.Size([64])
BlockList.1.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.1.residual_conv.bias 	 torch.Size([32])
BlockList.1.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.1.fcmy.0.bias 	 torch.Size([12])
BlockList.1.ln.weight 	 torch.Size([32])
BlockList.1.ln.bias 	 torch.Size([32])
BlockList.2.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.2.pre_conv.bias 	 torch.Size([512])
BlockList.2.EmbedT.pos_embed.weight 	 torch.Size([12, 358])
BlockList.2.EmbedT.norm.weight 	 torch.Size([358])
BlockList.2.EmbedT.norm.bias 	 torch.Size([358])
BlockList.2.EmbedS.pos_embed.weight 	 torch.Size([358, 512])
BlockList.2.EmbedS.norm.weight 	 torch.Size([512])
BlockList.2.EmbedS.norm.bias 	 torch.Size([512])
BlockList.2.TAt.W_Q.weight 	 torch.Size([96, 358])
BlockList.2.TAt.W_K.weight 	 torch.Size([96, 358])
BlockList.2.TAt.W_V.weight 	 torch.Size([96, 358])
BlockList.2.TAt.fc.weight 	 torch.Size([358, 96])
BlockList.2.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.2.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.2.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.mask.0 	 torch.Size([358, 358])
BlockList.2.cheb_conv_SAt.mask.1 	 torch.Size([358, 358])
BlockList.2.cheb_conv_SAt.mask.2 	 torch.Size([358, 358])
BlockList.2.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.2.gtu3.con2out.bias 	 torch.Size([64])
BlockList.2.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.2.gtu5.con2out.bias 	 torch.Size([64])
BlockList.2.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.2.gtu7.con2out.bias 	 torch.Size([64])
BlockList.2.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.2.residual_conv.bias 	 torch.Size([32])
BlockList.2.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.2.fcmy.0.bias 	 torch.Size([12])
BlockList.2.ln.weight 	 torch.Size([32])
BlockList.2.ln.bias 	 torch.Size([32])
BlockList.3.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.3.pre_conv.bias 	 torch.Size([512])
BlockList.3.EmbedT.pos_embed.weight 	 torch.Size([12, 358])
BlockList.3.EmbedT.norm.weight 	 torch.Size([358])
BlockList.3.EmbedT.norm.bias 	 torch.Size([358])
BlockList.3.EmbedS.pos_embed.weight 	 torch.Size([358, 512])
BlockList.3.EmbedS.norm.weight 	 torch.Size([512])
BlockList.3.EmbedS.norm.bias 	 torch.Size([512])
BlockList.3.TAt.W_Q.weight 	 torch.Size([96, 358])
BlockList.3.TAt.W_K.weight 	 torch.Size([96, 358])
BlockList.3.TAt.W_V.weight 	 torch.Size([96, 358])
BlockList.3.TAt.fc.weight 	 torch.Size([358, 96])
BlockList.3.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.3.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.3.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.mask.0 	 torch.Size([358, 358])
BlockList.3.cheb_conv_SAt.mask.1 	 torch.Size([358, 358])
BlockList.3.cheb_conv_SAt.mask.2 	 torch.Size([358, 358])
BlockList.3.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.3.gtu3.con2out.bias 	 torch.Size([64])
BlockList.3.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.3.gtu5.con2out.bias 	 torch.Size([64])
BlockList.3.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.3.gtu7.con2out.bias 	 torch.Size([64])
BlockList.3.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.3.residual_conv.bias 	 torch.Size([32])
BlockList.3.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.3.fcmy.0.bias 	 torch.Size([12])
BlockList.3.ln.weight 	 torch.Size([32])
BlockList.3.ln.bias 	 torch.Size([32])
final_conv.weight 	 torch.Size([128, 48, 1, 32])
final_conv.bias 	 torch.Size([128])
final_fc.weight 	 torch.Size([12, 128])
final_fc.bias 	 torch.Size([12])
Net's total params: 4172348
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]}]
current epoch:  0
validation batch 1 / 164, loss: 292.57
validation batch 101 / 164, loss: 218.26
val loss 178.79189970435166
best epoch:  0
best val loss:  178.79189970435166
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_0.params
current epoch:  1
validation batch 1 / 164, loss: 33.33
validation batch 101 / 164, loss: 29.23
val loss 21.05051705313892
best epoch:  1
best val loss:  21.05051705313892
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_1.params
current epoch:  2
validation batch 1 / 164, loss: 24.79
validation batch 101 / 164, loss: 25.77
val loss 17.924866539676014
best epoch:  2
best val loss:  17.924866539676014
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_2.params
global step: 1000, training loss: 17.93, time: 809.67s
current epoch:  3
validation batch 1 / 164, loss: 22.93
validation batch 101 / 164, loss: 23.74
val loss 16.871024954609755
best epoch:  3
best val loss:  16.871024954609755
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_3.params
current epoch:  4
validation batch 1 / 164, loss: 21.93
validation batch 101 / 164, loss: 22.49
val loss 16.22581708431244
best epoch:  4
best val loss:  16.22581708431244
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_4.params
global step: 2000, training loss: 16.59, time: 1591.47s
current epoch:  5
validation batch 1 / 164, loss: 21.14
validation batch 101 / 164, loss: 21.61
val loss 16.15201438927069
best epoch:  5
best val loss:  16.15201438927069
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_5.params
current epoch:  6
validation batch 1 / 164, loss: 20.79
validation batch 101 / 164, loss: 22.29
val loss 15.62696681080795
best epoch:  6
best val loss:  15.62696681080795
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_6.params
global step: 3000, training loss: 16.83, time: 2372.70s
current epoch:  7
validation batch 1 / 164, loss: 20.27
validation batch 101 / 164, loss: 21.85
val loss 15.559116959571838
best epoch:  7
best val loss:  15.559116959571838
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_7.params
current epoch:  8
validation batch 1 / 164, loss: 20.13
validation batch 101 / 164, loss: 21.10
val loss 15.519765100828032
best epoch:  8
best val loss:  15.519765100828032
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_8.params
global step: 4000, training loss: 14.48, time: 3150.10s
current epoch:  9
validation batch 1 / 164, loss: 19.87
validation batch 101 / 164, loss: 20.89
val loss 15.189809374692963
best epoch:  9
best val loss:  15.189809374692963
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_9.params
current epoch:  10
validation batch 1 / 164, loss: 19.76
validation batch 101 / 164, loss: 20.89
val loss 15.107650294536498
best epoch:  10
best val loss:  15.107650294536498
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_10.params
global step: 5000, training loss: 15.09, time: 3929.57s
current epoch:  11
validation batch 1 / 164, loss: 19.49
validation batch 101 / 164, loss: 21.47
val loss 14.893749495831932
best epoch:  11
best val loss:  14.893749495831932
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_11.params
current epoch:  12
validation batch 1 / 164, loss: 19.50
validation batch 101 / 164, loss: 20.93
val loss 14.893736656119184
best epoch:  12
best val loss:  14.893736656119184
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_12.params
global step: 6000, training loss: 15.63, time: 4665.14s
current epoch:  13
validation batch 1 / 164, loss: 19.25
validation batch 101 / 164, loss: 20.06
val loss 14.856665175135543
best epoch:  13
best val loss:  14.856665175135543
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_13.params
current epoch:  14
validation batch 1 / 164, loss: 19.34
validation batch 101 / 164, loss: 19.90
val loss 14.77593883363212
best epoch:  14
best val loss:  14.77593883363212
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_14.params
global step: 7000, training loss: 12.33, time: 5360.21s
current epoch:  15
validation batch 1 / 164, loss: 19.25
validation batch 101 / 164, loss: 20.84
val loss 14.605486768047983
best epoch:  15
best val loss:  14.605486768047983
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_15.params
current epoch:  16
validation batch 1 / 164, loss: 19.01
validation batch 101 / 164, loss: 20.73
val loss 14.61579831053571
global step: 8000, training loss: 14.75, time: 6055.32s
current epoch:  17
validation batch 1 / 164, loss: 19.09
validation batch 101 / 164, loss: 19.76
val loss 14.478299507280676
best epoch:  17
best val loss:  14.478299507280676
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_17.params
current epoch:  18
validation batch 1 / 164, loss: 18.91
validation batch 101 / 164, loss: 19.96
val loss 14.496940147585985
global step: 9000, training loss: 13.51, time: 6750.26s
current epoch:  19
validation batch 1 / 164, loss: 19.03
validation batch 101 / 164, loss: 19.98
val loss 14.46762535630203
best epoch:  19
best val loss:  14.46762535630203
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_19.params
current epoch:  20
validation batch 1 / 164, loss: 19.02
validation batch 101 / 164, loss: 20.75
val loss 14.417367914827858
best epoch:  20
best val loss:  14.417367914827858
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_20.params
global step: 10000, training loss: 14.31, time: 7445.31s
current epoch:  21
validation batch 1 / 164, loss: 19.07
validation batch 101 / 164, loss: 20.70
val loss 14.517104047100718
current epoch:  22
validation batch 1 / 164, loss: 18.92
validation batch 101 / 164, loss: 19.65
val loss 14.37033724784851
best epoch:  22
best val loss:  14.37033724784851
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_22.params
global step: 11000, training loss: 14.39, time: 8184.66s
current epoch:  23
validation batch 1 / 164, loss: 18.90
validation batch 101 / 164, loss: 19.82
val loss 14.289828224879939
best epoch:  23
best val loss:  14.289828224879939
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_23.params
current epoch:  24
validation batch 1 / 164, loss: 18.84
validation batch 101 / 164, loss: 19.42
val loss 14.239930170338328
best epoch:  24
best val loss:  14.239930170338328
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_24.params
global step: 12000, training loss: 14.30, time: 8965.19s
current epoch:  25
validation batch 1 / 164, loss: 18.89
validation batch 101 / 164, loss: 20.11
val loss 14.276356903518119
current epoch:  26
validation batch 1 / 164, loss: 18.77
validation batch 101 / 164, loss: 19.72
val loss 14.197180442693757
best epoch:  26
best val loss:  14.197180442693757
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_26.params
global step: 13000, training loss: 13.32, time: 9741.81s
current epoch:  27
validation batch 1 / 164, loss: 18.83
validation batch 101 / 164, loss: 20.51
val loss 14.327732339137938
current epoch:  28
validation batch 1 / 164, loss: 18.69
validation batch 101 / 164, loss: 19.72
val loss 14.154412845285927
best epoch:  28
best val loss:  14.154412845285927
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_28.params
global step: 14000, training loss: 14.60, time: 10524.13s
current epoch:  29
validation batch 1 / 164, loss: 18.97
validation batch 101 / 164, loss: 19.79
val loss 14.210714651317131
current epoch:  30
validation batch 1 / 164, loss: 18.67
validation batch 101 / 164, loss: 19.13
val loss 14.176430059642326
global step: 15000, training loss: 13.23, time: 11301.67s
current epoch:  31
validation batch 1 / 164, loss: 18.73
validation batch 101 / 164, loss: 19.45
val loss 14.21211798888881
current epoch:  32
validation batch 1 / 164, loss: 18.75
validation batch 101 / 164, loss: 19.23
val loss 14.153482486562032
best epoch:  32
best val loss:  14.153482486562032
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_32.params
global step: 16000, training loss: 12.34, time: 12080.38s
current epoch:  33
validation batch 1 / 164, loss: 18.76
validation batch 101 / 164, loss: 19.18
val loss 14.219987319736946
current epoch:  34
validation batch 1 / 164, loss: 18.74
validation batch 101 / 164, loss: 19.51
val loss 14.125435247653868
best epoch:  34
best val loss:  14.125435247653868
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_34.params
global step: 17000, training loss: 14.16, time: 12843.08s
current epoch:  35
validation batch 1 / 164, loss: 18.63
validation batch 101 / 164, loss: 19.40
val loss 14.04800666541588
best epoch:  35
best val loss:  14.04800666541588
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_35.params
current epoch:  36
validation batch 1 / 164, loss: 18.71
validation batch 101 / 164, loss: 19.03
val loss 14.154033349781502
global step: 18000, training loss: 13.40, time: 13538.88s
current epoch:  37
validation batch 1 / 164, loss: 18.59
validation batch 101 / 164, loss: 19.03
val loss 14.152094518266074
current epoch:  38
validation batch 1 / 164, loss: 18.69
validation batch 101 / 164, loss: 19.50
val loss 14.043033797566483
best epoch:  38
best val loss:  14.043033797566483
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_38.params
global step: 19000, training loss: 13.26, time: 14234.69s
current epoch:  39
validation batch 1 / 164, loss: 18.85
validation batch 101 / 164, loss: 18.99
val loss 14.161113826239982
current epoch:  40
validation batch 1 / 164, loss: 18.46
validation batch 101 / 164, loss: 19.69
val loss 14.006320956276685
best epoch:  40
best val loss:  14.006320956276685
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_40.params
global step: 20000, training loss: 12.93, time: 14930.82s
current epoch:  41
validation batch 1 / 164, loss: 18.59
validation batch 101 / 164, loss: 19.49
val loss 14.087689536373789
current epoch:  42
validation batch 1 / 164, loss: 18.51
validation batch 101 / 164, loss: 19.22
val loss 14.116014800420622
global step: 21000, training loss: 13.36, time: 15636.50s
current epoch:  43
validation batch 1 / 164, loss: 18.53
validation batch 101 / 164, loss: 19.63
val loss 14.058778376114077
current epoch:  44
validation batch 1 / 164, loss: 18.50
validation batch 101 / 164, loss: 19.30
val loss 13.953454320023699
best epoch:  44
best val loss:  13.953454320023699
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_44.params
global step: 22000, training loss: 13.81, time: 16413.15s
current epoch:  45
validation batch 1 / 164, loss: 18.38
validation batch 101 / 164, loss: 19.89
val loss 14.012701264241846
current epoch:  46
validation batch 1 / 164, loss: 18.50
validation batch 101 / 164, loss: 20.16
val loss 14.092572697779028
global step: 23000, training loss: 13.86, time: 17189.38s
current epoch:  47
validation batch 1 / 164, loss: 18.35
validation batch 101 / 164, loss: 19.27
val loss 13.974455836342603
current epoch:  48
validation batch 1 / 164, loss: 18.47
validation batch 101 / 164, loss: 19.92
val loss 13.99298481534167
global step: 24000, training loss: 14.47, time: 17976.26s
current epoch:  49
validation batch 1 / 164, loss: 18.37
validation batch 101 / 164, loss: 19.36
val loss 14.013942468457106
current epoch:  50
validation batch 1 / 164, loss: 18.35
validation batch 101 / 164, loss: 19.87
val loss 14.010003194576356
global step: 25000, training loss: 12.09, time: 18758.56s
current epoch:  51
validation batch 1 / 164, loss: 18.58
validation batch 101 / 164, loss: 18.73
val loss 13.971519679557986
current epoch:  52
validation batch 1 / 164, loss: 18.35
validation batch 101 / 164, loss: 19.00
val loss 13.971623112515706
global step: 26000, training loss: 14.21, time: 19537.09s
current epoch:  53
validation batch 1 / 164, loss: 18.37
validation batch 101 / 164, loss: 18.64
val loss 13.968831667085974
current epoch:  54
validation batch 1 / 164, loss: 18.48
validation batch 101 / 164, loss: 19.76
val loss 13.960850125405846
global step: 27000, training loss: 12.96, time: 20318.72s
current epoch:  55
validation batch 1 / 164, loss: 18.32
validation batch 101 / 164, loss: 19.19
val loss 14.011268266817419
current epoch:  56
validation batch 1 / 164, loss: 18.38
validation batch 101 / 164, loss: 19.09
val loss 13.974178058345144
current epoch:  57
validation batch 1 / 164, loss: 18.42
validation batch 101 / 164, loss: 18.96
val loss 13.95535800805906
global step: 28000, training loss: 12.14, time: 21132.62s
current epoch:  58
validation batch 1 / 164, loss: 18.30
validation batch 101 / 164, loss: 19.07
val loss 13.96505780627088
current epoch:  59
validation batch 1 / 164, loss: 18.48
validation batch 101 / 164, loss: 19.05
val loss 13.995129701567858
global step: 29000, training loss: 13.54, time: 21907.06s
current epoch:  60
validation batch 1 / 164, loss: 18.41
validation batch 101 / 164, loss: 18.84
val loss 13.984966862492445
current epoch:  61
validation batch 1 / 164, loss: 18.44
validation batch 101 / 164, loss: 18.99
val loss 13.936788887512392
best epoch:  61
best val loss:  13.936788887512392
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_61.params
global step: 30000, training loss: 12.29, time: 22681.91s
current epoch:  62
validation batch 1 / 164, loss: 18.26
validation batch 101 / 164, loss: 18.50
val loss 13.910987409149728
best epoch:  62
best val loss:  13.910987409149728
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_62.params
current epoch:  63
validation batch 1 / 164, loss: 18.28
validation batch 101 / 164, loss: 19.07
val loss 13.967053465726899
global step: 31000, training loss: 13.21, time: 23466.95s
current epoch:  64
validation batch 1 / 164, loss: 18.28
validation batch 101 / 164, loss: 19.18
val loss 13.899120967562606
best epoch:  64
best val loss:  13.899120967562606
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_64.params
current epoch:  65
validation batch 1 / 164, loss: 18.32
validation batch 101 / 164, loss: 19.04
val loss 13.952007744370437
global step: 32000, training loss: 12.10, time: 24245.85s
current epoch:  66
validation batch 1 / 164, loss: 18.36
validation batch 101 / 164, loss: 18.31
val loss 13.932673335075378
current epoch:  67
validation batch 1 / 164, loss: 18.31
validation batch 101 / 164, loss: 19.38
val loss 13.978058675440346
global step: 33000, training loss: 13.63, time: 25030.23s
current epoch:  68
validation batch 1 / 164, loss: 18.18
validation batch 101 / 164, loss: 19.26
val loss 13.90882501078815
current epoch:  69
validation batch 1 / 164, loss: 18.23
validation batch 101 / 164, loss: 19.01
val loss 13.86248690035285
best epoch:  69
best val loss:  13.86248690035285
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_69.params
global step: 34000, training loss: 13.19, time: 25813.08s
current epoch:  70
validation batch 1 / 164, loss: 18.24
validation batch 101 / 164, loss: 18.94
val loss 13.920011308135056
current epoch:  71
validation batch 1 / 164, loss: 18.31
validation batch 101 / 164, loss: 18.31
val loss 13.979515642654604
global step: 35000, training loss: 13.11, time: 26591.82s
current epoch:  72
validation batch 1 / 164, loss: 18.21
validation batch 101 / 164, loss: 18.98
val loss 13.884001121288392
current epoch:  73
validation batch 1 / 164, loss: 18.43
validation batch 101 / 164, loss: 18.85
val loss 13.964361999093033
global step: 36000, training loss: 12.25, time: 27369.62s
current epoch:  74
validation batch 1 / 164, loss: 18.46
validation batch 101 / 164, loss: 18.67
val loss 13.961769653529656
current epoch:  75
validation batch 1 / 164, loss: 18.40
validation batch 101 / 164, loss: 19.41
val loss 13.937569379806519
global step: 37000, training loss: 11.81, time: 28150.60s
current epoch:  76
validation batch 1 / 164, loss: 18.42
validation batch 101 / 164, loss: 19.13
val loss 13.910134446330186
current epoch:  77
validation batch 1 / 164, loss: 18.30
validation batch 101 / 164, loss: 18.95
val loss 13.911174256627152
global step: 38000, training loss: 12.54, time: 28931.30s
current epoch:  78
validation batch 1 / 164, loss: 18.42
validation batch 101 / 164, loss: 20.22
val loss 14.05761375950604
current epoch:  79
validation batch 1 / 164, loss: 18.31
validation batch 101 / 164, loss: 18.91
val loss 13.975523812014883
global step: 39000, training loss: 12.16, time: 29718.50s
current epoch:  80
validation batch 1 / 164, loss: 18.34
validation batch 101 / 164, loss: 18.90
val loss 13.896047001931725
current epoch:  81
validation batch 1 / 164, loss: 18.34
validation batch 101 / 164, loss: 19.08
val loss 13.892876715194888
global step: 40000, training loss: 11.70, time: 30500.54s
current epoch:  82
validation batch 1 / 164, loss: 18.37
validation batch 101 / 164, loss: 19.58
val loss 13.995149176295211
current epoch:  83
validation batch 1 / 164, loss: 18.30
validation batch 101 / 164, loss: 18.71
val loss 13.89520627987094
global step: 41000, training loss: 12.30, time: 31280.28s
current epoch:  84
validation batch 1 / 164, loss: 18.23
validation batch 101 / 164, loss: 18.40
val loss 13.902332483268365
current epoch:  85
validation batch 1 / 164, loss: 18.41
validation batch 101 / 164, loss: 19.27
val loss 13.981991119501068
global step: 42000, training loss: 11.61, time: 32057.39s
current epoch:  86
validation batch 1 / 164, loss: 18.21
validation batch 101 / 164, loss: 18.57
val loss 13.935541682127045
current epoch:  87
validation batch 1 / 164, loss: 18.24
validation batch 101 / 164, loss: 19.29
val loss 13.964529246818728
global step: 43000, training loss: 12.84, time: 32834.99s
current epoch:  88
validation batch 1 / 164, loss: 18.41
validation batch 101 / 164, loss: 19.07
val loss 13.900268540149781
current epoch:  89
validation batch 1 / 164, loss: 18.37
validation batch 101 / 164, loss: 19.19
val loss 13.89989042572859
global step: 44000, training loss: 13.07, time: 33618.33s
best epoch: 69
load weight from: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_69.params
predicting data set batch 1 / 164
predicting data set batch 101 / 164
input: (5237, 358, 1, 12)
prediction: (5237, 358, 12)
data_target_tensor: (5237, 358, 12)
current epoch: 69, predict 1-th point
MAE: 12.97
RMSE: 22.49
MAPE: 12.49
current epoch: 69, predict 2-th point
MAE: 13.75
RMSE: 23.92
MAPE: 13.21
current epoch: 69, predict 3-th point
MAE: 14.32
RMSE: 24.99
MAPE: 13.70
current epoch: 69, predict 4-th point
MAE: 14.76
RMSE: 25.83
MAPE: 14.00
current epoch: 69, predict 5-th point
MAE: 15.21
RMSE: 26.60
MAPE: 14.40
current epoch: 69, predict 6-th point
MAE: 15.66
RMSE: 27.33
MAPE: 14.81
current epoch: 69, predict 7-th point
MAE: 16.04
RMSE: 27.94
MAPE: 15.11
current epoch: 69, predict 8-th point
MAE: 16.31
RMSE: 28.38
MAPE: 15.34
current epoch: 69, predict 9-th point
MAE: 16.44
RMSE: 28.64
MAPE: 15.45
current epoch: 69, predict 10-th point
MAE: 16.55
RMSE: 28.84
MAPE: 15.58
current epoch: 69, predict 11-th point
MAE: 16.86
RMSE: 29.29
MAPE: 15.80
current epoch: 69, predict 12-th point
MAE: 17.56
RMSE: 30.22
MAPE: 16.44
all MAE: 15.54
all RMSE: 27.13
all MAPE: 14.69
[12.972296, 22.486594720825824, 12.494747340679169, 13.748576, 23.915002993185315, 13.212265074253082, 14.31794, 24.986059345584966, 13.695883750915527, 14.762464, 25.83317451018046, 14.001399278640747, 15.205034, 26.60002289964651, 14.395846426486969, 15.659087, 27.333384195916217, 14.806705713272095, 16.04043, 27.941324061898555, 15.106149017810822, 16.307226, 28.379262998876044, 15.336917340755463, 16.439621, 28.63867820204271, 15.450520813465118, 16.547293, 28.842805162951276, 15.581448376178741, 16.8645, 29.293468551960306, 15.80299288034439, 17.556753, 30.219509426858668, 16.43920987844467, 15.535107, 27.131495388057537, 14.693675935268402]

Process finished with exit code 0
